{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f61b0a1",
   "metadata": {},
   "source": [
    "# Final Project Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43dae8",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "434787d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "import functools\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.stattools import durbin_watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c8d154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trade_data = 'PATH_TO_TRADE_BOOK_DATA'\n",
    "iNAV_path = 'PATH_TO_INAV_DATA'\n",
    "where_to_save_data = 'PATH_TO_SAVE_LOCATION'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e5716",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75abee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_etf_df(etf_df):\n",
    "    \"\"\"take etf dataframe and clean it up.\"\"\"\n",
    "\n",
    "    etf_df.drop('SYM_SUFFIX',axis=1,inplace=True)\n",
    "    etf_df.dropna(inplace=True)\n",
    "\n",
    "    etf_df['DATE'] = pd.to_datetime(etf_df['DATE'],format='%Y%m%d')\n",
    "    etf_df['received'] = etf_df['DATE'].astype(str)+' '+etf_df['TIME_M']\n",
    "    etf_df['received'] = etf_df['received'].apply(pd.Timestamp)\n",
    "\n",
    "    etf_df['bid_ask_spread'] = etf_df['NBO']-etf_df['NBB']\n",
    "    etf_df['bid_ask_over_price'] = etf_df['bid_ask_spread']/etf_df['PRICE']\n",
    "    etf_df['bid_ask_over_price_timestamp_sum'] = etf_df.groupby('received')['bid_ask_over_price'].transform(sum)\n",
    "    etf_df['bid_ask_spread_timestamp_sum'] = etf_df.groupby('received')['bid_ask_spread'].transform(sum)\n",
    "    etf_df['dummy'] =1\n",
    "    etf_df['timestamp_count'] = etf_df.groupby('received')['dummy'].transform(sum) # trades per timestamp\n",
    "    etf_df['timestamp_volume'] = etf_df.groupby('received')['SIZE'].transform(sum) # volume in timestamp\n",
    "\n",
    "    etf_df.sort_values(['received','TR_SEQNUM'],inplace=True)\n",
    "    etf_df['cumulative_trade_count'] = etf_df['dummy'].cumsum()\n",
    "    etf_df['cumulative_volume'] = etf_df['SIZE'].cumsum() # cumulative volume\n",
    "\n",
    "    for i in [1,2]:  # 1 lag, then 2 lag\n",
    "        where0 = (etf_df.LeeReady==0)\n",
    "        lag = etf_df.shift(i)\n",
    "        # curr price is lower than prev price (seller initiated (-1))\n",
    "        etf_df[f\"lag{i}\"] = (etf_df[where0].PRICE < lag[where0].PRICE) * -1\n",
    "        # curr price is higher than prev price (buyer initiated (+1))\n",
    "        etf_df[f\"lag{i}\"] += (etf_df[where0].PRICE > lag[where0].PRICE) * 1\n",
    "        etf_df[f\"lag{i}\"] = etf_df[f\"lag{i}\"].fillna(0)\n",
    "\n",
    "        etf_df.LeeReady += etf_df[f\"lag{i}\"]\n",
    "\n",
    "    etf_df = etf_df.drop(columns=[f\"lag{i}\"])\n",
    "\n",
    "    etf_df['direction_size'] = etf_df['SIZE']*etf_df['LeeReady']\n",
    "    etf_df['direction_size'] = etf_df.groupby('received')['direction_size'].transform(sum) # sum per timestamp\n",
    "    \n",
    "    etf_df['dollar_direction'] = etf_df['PRICE']*etf_df['SIZE']*etf_df['LeeReady']\n",
    "    etf_df['dollar_direction'] = etf_df.groupby('received')['dollar_direction'].transform(sum) # sum per timestamp\n",
    "    \n",
    "    etf_df = etf_df.groupby('received').tail(1) # take last trade per timestamp\n",
    "    etf_df.set_index('received',inplace=True) # set index to received timestamp\n",
    "\n",
    "    time_criteria = (((etf_df.index.hour >= 9)&(etf_df.index.minute >= 30))|\n",
    "                     (etf_df.index.hour>9))&(etf_df.index.hour < 16)\n",
    "\n",
    "    etf_df = etf_df.loc[time_criteria].sort_index()\n",
    "    \n",
    "    etf_df['order_imbalance'] = (etf_df['NBBqty']-etf_df['NBOqty'])/(etf_df['NBBqty']+etf_df['NBOqty'])\n",
    "    \n",
    "    return etf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b53d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tau_sum(data_series,Tau):\n",
    "    \"\"\"Find cumulative sum fo values in the prior Tau time interval.\"\"\"\n",
    "    cumulative_sum = data_series.cumsum()\n",
    "    \n",
    "    T_cumulative_sum = cumulative_sum.reindex(cumulative_sum.index-pd.Timedelta(Tau), method='bfill')\n",
    "    T_cumulative_sum.index = cumulative_sum.index\n",
    "\n",
    "    sum_in_period = cumulative_sum-T_cumulative_sum\n",
    "    \n",
    "    return sum_in_period\n",
    "\n",
    "def tau_average(field,trade_counts,Tau):\n",
    "    field_cum = field.cumsum()\n",
    "    T_field_cum = field_cum.reindex(field_cum.index-pd.Timedelta(Tau), method='bfill')\n",
    "    T_field_cum.index = field_cum.index\n",
    "\n",
    "    field_sum_in_period = field_cum-T_field_cum\n",
    "    trades_in_period = tau_sum(trade_counts,Tau)\n",
    "\n",
    "    tau_mean_field = field_sum_in_period/trades_in_period\n",
    "    \n",
    "    return tau_mean_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6607f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_ewm_vol(price_series,T):\n",
    "    \"\"\"Calculate cum log returns up to and not including T.\n",
    "        EWMVar: Mean squared T return with decay = T.\n",
    "        Vol = square root of EWMVar.\"\"\"\n",
    "    #avoid jumps in the return series from close to open (could remove if we want to include these)\n",
    "    new_day = np.where(price_series.reset_index()['received'].dt.day.diff()!=0,np.nan,1)\n",
    "    returns = np.log(price_series/price_series.shift()) * new_day\n",
    "    T_returns = returns.rolling(T,closed='left').sum()\n",
    "    \n",
    "    ewm_var = (T_returns**2).ewm(halflife = T, times=returns.index,ignore_na=True).mean()\n",
    "    ewm_vol = np.sqrt(ewm_var)\n",
    "    ewm_vol.name = 'exp weighted volatility'\n",
    "    return ewm_vol\n",
    "\n",
    "def calc_volumes(trade_sizes):\n",
    "    \"\"\"Calculate total volume for Tau time period\"\"\"\n",
    "    \n",
    "    volumes = pd.concat(\n",
    "        {'volume_1min':tau_sum(trade_sizes,'60s'),\n",
    "         'volume_2min':tau_sum(trade_sizes,'120s'),\n",
    "         'volume_4min':tau_sum(trade_sizes,'240s'),\n",
    "         'volume_5min':tau_sum(trade_sizes,'300s'),\n",
    "         'volume_10min':tau_sum(trade_sizes,'600s'),\n",
    "         'volume_15min':tau_sum(trade_sizes,'900s'),\n",
    "         'volume_30min':tau_sum(trade_sizes,'1800s')},\n",
    "        axis=1)\n",
    "\n",
    "    return volumes\n",
    "\n",
    "def calc_tau_vwap(trade_prices,trade_sizes, Tau):\n",
    "    \"\"\"calculate volume weighted average price over tau time period\"\"\"\n",
    "    tau_volume = tau_sum(trade_sizes,Tau)\n",
    "    tau_total_dollar_size = tau_sum(trade_prices*trade_sizes,Tau)\n",
    "    \n",
    "    vwap = tau_total_dollar_size/tau_volume\n",
    "    \n",
    "    return vwap\n",
    "\n",
    "\n",
    "def T_fwd_rtn(price_series,T):\n",
    "    \"\"\"Calculate T forward returns\"\"\"\n",
    "\n",
    "    T_fwd_prices = price_series.reindex(price_series.index+pd.Timedelta(T), method='ffill')\n",
    "    T_fwd_prices.index = price_series.index\n",
    "    T_fwd_rtns = T_fwd_prices/price_series-1\n",
    "\n",
    "    return T_fwd_rtns\n",
    "\n",
    "def fwd_volume(trade_sizes,Tau):\n",
    "    \"\"\"calculate vwap for next Tau time period\"\"\"\n",
    "    trade_sizes_sum = trade_sizes.cumsum()\n",
    "\n",
    "    T_trade_sizes_sum = trade_sizes_sum.reindex(trade_sizes_sum.index+pd.Timedelta(Tau), method='ffill')\n",
    "    T_trade_sizes_sum.index = trade_sizes_sum.index\n",
    "    T_fwd_total_volume = (T_trade_sizes_sum-trade_sizes_sum) # if there were no trades, take previous price\n",
    "\n",
    "    T_fwd_total_volume.index = trade_sizes_sum.index\n",
    "    \n",
    "    return T_fwd_total_volume\n",
    "\n",
    "\n",
    "\n",
    "def fwd_vwap(trade_prices, trade_sizes,Tau):\n",
    "    \"\"\"calculate vwap for next Tau time period\"\"\"\n",
    "    trade_sizes_sum = trade_sizes.cumsum()\n",
    "\n",
    "    T_trade_sizes_sum = trade_sizes_sum.reindex(trade_sizes_sum.index+pd.Timedelta(Tau), method='ffill')\n",
    "    T_trade_sizes_sum.index = trade_sizes_sum.index\n",
    "    T_fwd_total_volume = (T_trade_sizes_sum-trade_sizes_sum) # if there were no trades, take previous price\n",
    "\n",
    "    price_times_size = trade_prices*trade_sizes\n",
    "    price_times_size_sum = price_times_size.cumsum()\n",
    "\n",
    "    T_price_times_size_sum = price_times_size_sum.reindex(price_times_size_sum.index+pd.Timedelta(Tau), method='ffill')\n",
    "    T_price_times_size_sum.index = price_times_size_sum.index\n",
    "    T_price_times_size_sum_total = T_price_times_size_sum - price_times_size_sum\n",
    "\n",
    "    fwd_vwap = T_price_times_size_sum_total/T_fwd_total_volume\n",
    "    \n",
    "    return fwd_vwap\n",
    "\n",
    "\n",
    "def vwap_fwd_rtn(trade_prices, trade_sizes, T, vwap_tau):\n",
    "    \"\"\"calculate vwap forward return\"\"\"\n",
    "    \n",
    "    T_fwd_prices = trade_prices.reindex(trade_prices.index+pd.Timedelta(T), method='ffill')\n",
    "    T_fwd_sizes = trade_sizes.reindex(trade_sizes.index+pd.Timedelta(T), method='ffill')\n",
    "    \n",
    "    T_fwd_vwap = fwd_vwap(trade_prices = T_fwd_prices, trade_sizes = T_fwd_sizes, Tau = vwap_tau) # fill with previous price\n",
    "    T_fwd_vwap = T_fwd_vwap.fillna(method='ffill')\n",
    "    \n",
    "    T_fwd_vwap.index = trade_prices.index\n",
    "    \n",
    "    start_vwap = fwd_vwap(trade_prices = trade_prices, trade_sizes=trade_sizes,Tau = vwap_tau) # fill with previous price\n",
    "    start_vwap = start_vwap.fillna(method='ffill')\n",
    "    \n",
    "    \n",
    "    vwap_fwd_rtns = T_fwd_vwap/start_vwap-1\n",
    "    \n",
    "    return vwap_fwd_rtns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12c44dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fwd_rtns(etf_trades):\n",
    "    \"Calculate different forward return periods\"\n",
    "    fwd_rtns = pd.concat(\n",
    "        {'fwd_rtn_5min':T_fwd_rtn(etf_trades['PRICE'],'300s'),\n",
    "         'fwd_rtn_10min':T_fwd_rtn(etf_trades['PRICE'],'600s'),\n",
    "         'fwd_rtn_15min':T_fwd_rtn(etf_trades['PRICE'],'900s')},\n",
    "        axis=1)\n",
    "    \n",
    "    return fwd_rtns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5dbc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ewm_vol(price_series):\n",
    "    \"\"\"Calculate EWMA Volatility Metrics\"\"\"\n",
    "    \n",
    "    intervals = [1,2,4,5,15,20,25,30,45,60]\n",
    "    times = [str(60*t)+'s' for t in intervals]\n",
    "    vols = pd.DataFrame(index=price_series.index)\n",
    "    for time,interval in zip(times,intervals):\n",
    "        annualize = 6.5*(60/interval)*252\n",
    "        vols['ewm_vol_'+time] = np.sqrt(T_ewm_vol(price_series,time)**2*annualize).replace(0,np.nan)\n",
    "    \n",
    "    return vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b6c6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_flow_metrics(sized_directions,dollar_sized_directions):\n",
    "    \"\"\"Calculate Flow Metrics\"\"\"\n",
    "    \n",
    "    dollar_flows = pd.concat(\n",
    "        {'dollar_flow_1min':tau_sum(dollar_sized_directions,'60s'),\n",
    "         'dollar_flow_2min':tau_sum(dollar_sized_directions,'120s'),\n",
    "         'dollar_flow_4min':tau_sum(dollar_sized_directions,'240s'),\n",
    "         'dollar_flow_5min':tau_sum(dollar_sized_directions,'300s'),\n",
    "         'dollar_flow_15min':tau_sum(dollar_sized_directions,'900s'),\n",
    "         'dollar_flow_20min':tau_sum(dollar_sized_directions,'1200s'),\n",
    "         'dollar_flow_25min':tau_sum(dollar_sized_directions,'1500s'),\n",
    "         'dollar_flow_30min':tau_sum(dollar_sized_directions,'1800s'),\n",
    "         'dollar_flow_45min':tau_sum(dollar_sized_directions,'2700s'),\n",
    "         'dollar_flow_60min':tau_sum(dollar_sized_directions,'3600s')},\n",
    "        axis=1)\n",
    "    \n",
    "    flows = pd.concat(\n",
    "        {'flow_1min':tau_sum(sized_directions,'60s'),\n",
    "         'flow_2min':tau_sum(sized_directions,'120s'),\n",
    "         'flow_4min':tau_sum(sized_directions,'240s'),\n",
    "         'flow_5min':tau_sum(sized_directions,'300s'),\n",
    "         'flow_15min':tau_sum(sized_directions,'900s'),\n",
    "         'flow_20min':tau_sum(sized_directions,'1200s'),\n",
    "         'flow_25min':tau_sum(sized_directions,'1500s'),\n",
    "         'flow_30min':tau_sum(sized_directions,'1800s'),\n",
    "         'flow_45min':tau_sum(sized_directions,'2700s'),\n",
    "         'flow_60min':tau_sum(sized_directions,'3600s')},\n",
    "        axis=1)\n",
    "\n",
    "    EWMA_flows = pd.concat(\n",
    "        {'flow_1min_EWMA':flows['flow_1min'].ewm(halflife='120s',times=flows.index).mean(),\n",
    "         'flow_2min_EWMA':flows['flow_2min'].ewm(halflife='240s',times=flows.index).mean(),\n",
    "         'flow_4min_EWMA':flows['flow_4min'].ewm(halflife='480s',times=flows.index).mean(),\n",
    "         'flow_5min_EWMA':flows['flow_5min'].ewm(halflife='600s',times=flows.index).mean(),\n",
    "         'flow_15min_EWMA':flows['flow_15min'].ewm(halflife='1800s',times=flows.index).mean(),\n",
    "         'flow_20min_EWMA':flows['flow_20min'].ewm(halflife='2400s',times=flows.index).mean(),\n",
    "         'flow_25min_EWMA':flows['flow_25min'].ewm(halflife='3000s',times=flows.index).mean(),\n",
    "         'flow_30min_EWMA':flows['flow_30min'].ewm(halflife='3600s',times=flows.index).mean(),\n",
    "         'flow_45min_EWMA':flows['flow_45min'].ewm(halflife='5400s',times=flows.index).mean(),\n",
    "         'flow_60min_EWMA':flows['flow_60min'].ewm(halflife='7200s',times=flows.index).mean()},\n",
    "        axis=1)\n",
    "\n",
    "    all_flows = flows.join([EWMA_flows,dollar_flows])\n",
    "    \n",
    "    return all_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2221f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bid_ask_metrics(bid_ask_spread_sum, bid_ask_over_price_sum, trade_counts):\n",
    "    \"\"\"Calculate average bid ask spread and average bid ask spread divided by price variables\"\"\"\n",
    "    bid_ask = pd.concat(\n",
    "        {'bid_ask_1min':tau_average(field=bid_ask_spread_sum, trade_counts=trade_counts, Tau='60s'),\n",
    "         'bid_ask_2min':tau_average(field=bid_ask_spread_sum, trade_counts=trade_counts, Tau='120s'),\n",
    "         'bid_ask_5min':tau_average(field=bid_ask_spread_sum, trade_counts=trade_counts, Tau='300s'),\n",
    "         'bid_ask_10min':tau_average(field=bid_ask_spread_sum, trade_counts=trade_counts, Tau='600s'),\n",
    "         'bid_ask_15min':tau_average(field=bid_ask_spread_sum, trade_counts=trade_counts, Tau='900s'),\n",
    "         'bid_ask_30min':tau_average(field=bid_ask_spread_sum, trade_counts=trade_counts, Tau='1800s'),\n",
    "         'bid_ask_60min':tau_average(field=bid_ask_spread_sum, trade_counts=trade_counts, Tau='3600s')},\n",
    "    axis=1) \n",
    "    \n",
    "    bid_ask_prices = pd.concat(\n",
    "        {'bid_ask_price_1min':tau_average(field=bid_ask_over_price_sum,trade_counts=trade_counts,Tau='60s'),\n",
    "         'bid_ask_price_2min':tau_average(field=bid_ask_over_price_sum,trade_counts=trade_counts,Tau='120s'),\n",
    "         'bid_ask_price_5min':tau_average(field=bid_ask_over_price_sum,trade_counts=trade_counts,Tau='300s'),\n",
    "         'bid_ask_price_10min':tau_average(field=bid_ask_over_price_sum,trade_counts=trade_counts,Tau='600s'),\n",
    "         'bid_ask_price_15min':tau_average(field=bid_ask_over_price_sum,trade_counts=trade_counts,Tau='900s'),\n",
    "         'bid_ask_price_30min':tau_average(field=bid_ask_over_price_sum,trade_counts=trade_counts,Tau='1800s'),\n",
    "         'bid_ask_price_60min':tau_average(field=bid_ask_over_price_sum,trade_counts=trade_counts,Tau='3600s')},\n",
    "        axis=1)\n",
    "\n",
    "    bid_ask_met = bid_ask_prices.join(bid_ask)\n",
    "    \n",
    "    return bid_ask_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96e378eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_imbalance_metrics(imbalances, trade_counts):\n",
    "    \"\"\"Calculate tau time period average order imbalances. for 1,2,4,5,10,15,30 minutes\"\"\"\n",
    "    \n",
    "    order_imbalances = pd.concat(\n",
    "        {'order_imbalance_1min':tau_average(imbalances, trade_counts, '60s'),\n",
    "         'order_imbalance_2min':tau_average(imbalances, trade_counts, '120s'),\n",
    "         'order_imbalance_4min':tau_average(imbalances, trade_counts, '240s'),\n",
    "         'order_imbalance_5min':tau_average(imbalances, trade_counts, '300s'),\n",
    "         'order_imbalance_10min':tau_average(imbalances, trade_counts, '600s'),\n",
    "         'order_imbalance_15min':tau_average(imbalances, trade_counts, '900s'),\n",
    "         'order_imbalance_30min':tau_average(imbalances, trade_counts, '1800s')},\n",
    "        axis=1)\n",
    "    \n",
    "    return order_imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51ac61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_all_metrics(etf_trades, iNAV):\n",
    "    \"\"\"Calculate all the trade data metrics, forward returns, and join it together.\"\"\"\n",
    "    etf_trades = clean_etf_df(etf_trades)\n",
    "\n",
    "    flow_metrics = calc_flow_metrics(sized_directions = etf_trades['direction_size'],\n",
    "                                     dollar_sized_directions = etf_trades['dollar_direction'])\n",
    "\n",
    "    bid_ask_metrics = calc_bid_ask_metrics(bid_ask_spread_sum = etf_trades['bid_ask_spread_timestamp_sum'],\n",
    "                                           bid_ask_over_price_sum = etf_trades['bid_ask_over_price_timestamp_sum'],\n",
    "                                           trade_counts = etf_trades['timestamp_count'])\n",
    "    \n",
    "    volatilities = calc_ewm_vol(price_series = etf_trades['PRICE'])\n",
    "    volumes = calc_volumes(trade_sizes = etf_trades['timestamp_volume'])\n",
    "    order_imbalances = calc_imbalance_metrics(etf_trades['order_imbalance'], trade_counts=etf_trades['timestamp_count'])\n",
    "    \n",
    "    fields_from_trade_book = etf_trades.loc[:,['PRICE','NBB','NBO','NBOqty','NBBqty',\n",
    "                                               'cumulative_trade_count','cumulative_volume','order_imbalance']]\n",
    "    \n",
    "    \n",
    "    # join all the fields from trade book together\n",
    "    trade_book_variables = flow_metrics.join([bid_ask_metrics,\n",
    "                                              volatilities,\n",
    "                                              volumes,\n",
    "                                              order_imbalances,\n",
    "                                              fields_from_trade_book])\n",
    "    \n",
    "    independent_variables = pd.merge_asof(iNAV,\n",
    "                                          trade_book_variables,\n",
    "                                          left_index=True,\n",
    "                                          right_index=True,\n",
    "                                          direction='backward',\n",
    "                                          allow_exact_matches=True)\n",
    "\n",
    "    independent_variables['nav_discount_bid'] = independent_variables['NBB']/independent_variables['iNAV']-1\n",
    "    independent_variables['nav_discount_ask'] = independent_variables['NBO']/independent_variables['iNAV']-1\n",
    "    \n",
    "    # forward returns\n",
    "    fwd_rtns = calc_fwd_rtns(etf_trades)\n",
    "    \n",
    "    vwap_fwd_rtns = pd.concat(\n",
    "       {'vwap_fwd_rtn_5min_10s_vwap':vwap_fwd_rtn(trade_prices=etf_trades['PRICE'],\n",
    "                                                  trade_sizes = etf_trades['timestamp_volume'],\n",
    "                                                  T='600s',\n",
    "                                                  vwap_tau = '10s'),\n",
    "       'fwd_vwap_10s':fwd_vwap(trade_prices = etf_trades['PRICE'], \n",
    "                                 trade_sizes=etf_trades['timestamp_volume'],\n",
    "                                 Tau = '10s'),\n",
    "       'fwd_volume_10s':fwd_volume(trade_sizes=etf_trades['timestamp_volume'],\n",
    "                                   Tau = '10s')},\n",
    "       axis=1)\n",
    "    \n",
    "    all_fwd_rtns = fwd_rtns.join(vwap_fwd_rtns)\n",
    "    all_fwd_rtns.index = all_fwd_rtns.index - pd.Timedelta(50,'milli') # subtract 50 milliseconds for latency\n",
    "\n",
    "    all_vars = pd.merge_asof(independent_variables,\n",
    "                             all_fwd_rtns,\n",
    "                             left_index=True,\n",
    "                             right_index=True,\n",
    "                             direction='forward',\n",
    "                             allow_exact_matches=True)\n",
    "    \n",
    "    return all_vars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca324a",
   "metadata": {},
   "source": [
    "#### HYG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acaa4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNAV = pd.read_csv(iNAV_path+'hygiv.csv', index_col = 'date',parse_dates=True).sort_index()\n",
    "\n",
    "vars_1920 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'HYG_1920.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year < 2021].copy())\n",
    "\n",
    "vars_1920.to_csv(where_to_save_data+'hyg_metrics_1920.csv')\n",
    "\n",
    "del vars_1920\n",
    "\n",
    "vars_2021 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'HYG_2021.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year == 2021].copy())\n",
    "\n",
    "vars_2021.to_csv(where_to_save_data+'hyg_metrics_2021.csv')\n",
    "\n",
    "del vars_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55906c",
   "metadata": {},
   "source": [
    "#### JNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ba658b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNAV = pd.read_csv(iNAV_path+'jnkiv.csv', index_col = 'date',parse_dates=True).sort_index()\n",
    "\n",
    "vars_1920 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'JNK_1920.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year < 2021].copy())\n",
    "\n",
    "vars_1920.to_csv(where_to_save_data+'jnk_metrics_1920.csv')\n",
    "\n",
    "del vars_1920\n",
    "\n",
    "vars_2021 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'JNK_2021.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year == 2021].copy())\n",
    "\n",
    "vars_2021.to_csv(where_to_save_data+'jnk_metrics_2021.csv')\n",
    "\n",
    "del vars_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db43f2",
   "metadata": {},
   "source": [
    "#### BKLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d50a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNAV = pd.read_csv(iNAV_path+'bklniv.csv', index_col = 'date',parse_dates=True).sort_index()\n",
    "\n",
    "vars_1920 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'BKLN_1920.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year < 2021].copy())\n",
    "\n",
    "vars_1920.to_csv(where_to_save_data+'bkln_metrics_1920.csv')\n",
    "\n",
    "del vars_1920\n",
    "\n",
    "vars_2021 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'BKLN_2021.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year == 2021].copy())\n",
    "\n",
    "vars_2021.to_csv(where_to_save_data+'bkln_metrics_2021.csv')\n",
    "\n",
    "del vars_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bb91d",
   "metadata": {},
   "source": [
    "#### SRLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b6c45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNAV = pd.read_csv(iNAV_path+'srlniv.csv', index_col = 'date',parse_dates=True).sort_index()\n",
    "\n",
    "vars_1920 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'SRLN_1920.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year < 2021].copy())\n",
    "\n",
    "vars_1920.to_csv(where_to_save_data+'srln_metrics_1920.csv')\n",
    "\n",
    "del vars_1920\n",
    "\n",
    "vars_2021 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'SRLN_2021.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year == 2021].copy())\n",
    "\n",
    "vars_2021.to_csv(where_to_save_data+'srln_metrics_2021.csv')\n",
    "\n",
    "del vars_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b67eed",
   "metadata": {},
   "source": [
    "#### PFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d55526d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNAV = pd.read_csv(iNAV_path+'pffiv.csv', index_col = 'date',parse_dates=True).sort_index()\n",
    "\n",
    "vars_1920 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'PFF_1920.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year < 2021].copy())\n",
    "\n",
    "vars_1920.to_csv(where_to_save_data+'pff_metrics_1920.csv')\n",
    "\n",
    "del vars_1920\n",
    "\n",
    "vars_2021 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'PFF_2021.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year == 2021].copy())\n",
    "\n",
    "vars_2021.to_csv(where_to_save_data+'pff_metrics_2021.csv')\n",
    "\n",
    "del vars_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d5979",
   "metadata": {},
   "source": [
    "#### PGX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb7bd6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNAV = pd.read_csv(iNAV_path+'pgxiv.csv', index_col = 'date',parse_dates=True).sort_index()\n",
    "\n",
    "vars_1920 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'PGX_1920.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year < 2021].copy())\n",
    "\n",
    "vars_1920.to_csv(where_to_save_data+'pgx_metrics_1920.csv')\n",
    "\n",
    "del vars_1920\n",
    "\n",
    "vars_2021 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'PGX_2021.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year == 2021].copy())\n",
    "\n",
    "vars_2021.to_csv(where_to_save_data+'pgx_metrics_2021.csv')\n",
    "\n",
    "del vars_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295623e9",
   "metadata": {},
   "source": [
    "#### SPHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc17a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNAV = pd.read_csv(iNAV_path+'sphyiv.csv', index_col = 'date',parse_dates=True).sort_index()\n",
    "\n",
    "vars_1920 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'SPHY_1920.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year < 2021].copy())\n",
    "\n",
    "vars_1920.to_csv(where_to_save_data+'sphy_metrics_1920.csv')\n",
    "\n",
    "del vars_1920\n",
    "\n",
    "vars_2021 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'SPHY_2021.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year == 2021].copy())\n",
    "\n",
    "vars_2021.to_csv(where_to_save_data+'sphy_metrics_2021.csv')\n",
    "\n",
    "del vars_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a93b7",
   "metadata": {},
   "source": [
    "#### HYGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5560dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNAV = pd.read_csv(iNAV_path+'hyghiv.csv', index_col = 'date',parse_dates=True).sort_index()\n",
    "\n",
    "vars_1920 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'HYGH_1920.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year < 2021].copy())\n",
    "\n",
    "vars_1920.to_csv(where_to_save_data+'hygh_metrics_1920.csv')\n",
    "\n",
    "del vars_1920\n",
    "\n",
    "vars_2021 = calc_all_metrics(etf_trades = pd.read_csv(path_to_trade_data+'HYGH_2021.csv'),\n",
    "                            iNAV=iNAV.loc[iNAV.index.year == 2021].copy())\n",
    "\n",
    "vars_2021.to_csv(where_to_save_data+'hygh_metrics_2021.csv')\n",
    "\n",
    "del vars_2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
